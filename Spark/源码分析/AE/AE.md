
## 一

大家好，今天我给大家分享的内容是“Spark自适应执行引擎”。

## 二

首先我会介绍一下SparkSQL面临的挑战以及自适应执行要解决的问题，然后讲一下自适应执行的背景和架构，最后展示一个官方的性能测试。

## 三

SparkSQL在大规模数据集上遇到的挑战有以下三点：一是“shuffle partition数量是固定不可变的”；二是“执行计划是静态无法变更的”；三是数据倾斜问题。下面我们详细看一下这三个问题的影响。


## 四

我们在配置shffle partition数量时，是通过设置参数spark.sql.shuffle.partition，它表示每个reduce阶段的任务数，该值默认为200。

一个spark作业可使用的cpu核心数是：executor的数量乘以每个Executor的core数，它表示可以并发执行的任务数。

那么每个reduce stage需要执行P/C这么多轮才可以完成。

这里P代表了task数量，P的设置就对整个查询性能有很大的影响。

这里面临的第一个大的问题是P的值不好确定：

- 如果P设置过小，分配给每一个reduce任务“处理的数据量”就越多，在内存大小有限的情况下，就必须spill到磁盘上。Spill会导致额外的磁盘读写，影响整个SQL查询的性能，更差的情况还可能导致GC问题甚至是OOM。
- 如果P设置过大，那么单个任务处理的数据量很小并且很快结束，导致Spark任务调度负担加重。
另外一点就是，当mapper把数据输出到hash bucket里时，hash bucket里的数据量会很小。那么reducer在拉取数据时会进行随机小数据的读操作，就会导致更多的IO请求。
再有一点就是，在最后一个stage阶段，数据被写到P个文件里，可能会产生大量的小文件。

第二个大的问题是：

每个stage的数据分布和大小可能都不太一样，全局的shuffle partition设置最多只能对某个或者某些stage最优，没有办法做到对于“全局所有stage”都是最优的。

由此引发我们思考一个问题：能否自动为每一个stage设置合适的shuffle partition值？


## 五

第二个挑战：静态的执行计划

这张图是Spark SQL的整体架构。

Spark SQL在执行之前，会先生成逻辑计划，再生成一个可执行的物理计划。执行计划一旦确定以后就不能改变。

但是在运行期间，可以获得更多的运行时信息，根据运行时信息，是有可能得到一个更好的执行计划的，所以动态变更执行计划会是一个更好的优化手段。

由此引发我们思考第二个问题：能否通过收集运行时信息，动态调整执行计划？


## 六

我们来看一下这张图，这是一个join操作，右边的shuffle write的数据有436G，左边的shuffle write数据只有46.9K，比较好的处理方式是，将小表广播到每一个executor上，然后在map阶段，每一个mapper读取大表的一个分片，并且和整张小表进行join，整个过程中避免了把大表的数据进行shuffle。

但是SparkSQL在生成执行计划时，并不能精确地知道join中两表的大小，或者错误地估计它们的大小，所以有可能选择SortMergeJoin这样的操作。如果能在运行过程中得到两个表的精确大小，就可以使用BroadcastHashJoin，这样性能会更好。

## 七

第三个挑战是数据倾斜问题。

数据倾斜是指，某一个partition的数据量远远大于其它partition的数据量，导致个别任务的运行时间远远大于其它任务，因此拖累了整个SQL的运行时间。

目前数据倾斜的解决办法有：
- 增大shuffle partition数量
- 增大BroadcastHashJoin阈值，将ShuffleJoin转化成BroadcastJoin
- 给倾斜的key增加随机的前缀

以及其他一些方法。

但是数据倾斜的这些解决办法都有各自的局限性，并且涉及很多的人为处理。

由此引发我们思考第三个问题：能否在运行时自动地处理join中的数据倾斜？


## 八

早在2015年，Spark社区就提出了自适应执行的基本想法，在Spark的DAGScheduler中增加了“提交单个map stage的接口”，并且在“运行时调整shuffle partition数量”这一方面做了尝试。但目前这个实现有一定的局限性，在某些场景下会引入更多的shuffle，对于三表在同一个stage中做join等情况也无法很好的处理。所以这个功能一直处于实验阶段，配置参数也没有在官方文档中提及。

基于这些社区的工作，英特尔大数据团队对自适应执行做了重新设计，实现了一个更为灵活的自适性执行框架。在这个框架里面，可以添加额外的规则，来实现更多的功能。目前，已实现的特性包括：自动设置shuffle partition数量，动态调整执行计划，动态处理数据倾斜。

## 九

这张图描述了自适应执行的架构。

Spark SQL原先的处理流程是：在确定了最后的物理执行计划后，根据每一个operator对RDD的转换定义，它会生成一个RDD的DAG图。之后基于DAG图静态划分stage并且提交执行，所以一旦执行计划确定后，在运行阶段就没法再变更了。

自适应执行的基本思路是：在执行计划中事先划分好stage，然后按stage提交执行，在运行时收集当前stage的shuffle统计信息，以此来优化下一个stage的执行计划，然后再提交后续的stage。 这样就实现了执行计划的动态调整，而且调整的依据是“中间结果的精确统计信息”。

那么它具体是怎么做的呢？

我们看到图里有Exchange节点，Exchange节点在Spark SQL中代表shuffle。

以Exchange节点作为分界将执行计划划分成多个QueryStage。每一个QueryStage都是一棵独立的子树，也是一个独立的执行单元。在加入QueryStage的同时，也加入一个QueryStageInput叶子节点，作为父亲QueryStage的输入。在执行QueryStage时，首先提交它的child stage，并且收集这些stage的运行时信息。当这些child stage运行完毕后，就可以得到它们的大小、行数等信息，以此来判断“QueryStage中的执行计划”是否可以优化更新。


## 十

最后展示一下官方的一个测试数据。

这是基于Spark 2.2，在100TB的“TPC-DS数据集”上作的性能对比，蓝色的表示原生spark的执行时间，橙色的表示AE版本的执行时间。

横轴是SQL语句ID，纵轴是执行时间。

图中选取了性能提升最大的几条sql语句。

实验结果显示，103条SQL语句中，有92条都有了明显的性能提升，其中47条语句的性能提升超过10%，最大的性能提升达到了3.8倍，并且没有出现性能下降的情况。


